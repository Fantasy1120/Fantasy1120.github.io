<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="HV-MMBench">
  <meta name="keywords" content="T2V, Video Generation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>HV-MMBench</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./assets/css/bulma.min.css">
  <link rel="stylesheet" href="./assets/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./assets/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./assets/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./assets/css/index.css">
  <!-- <link rel="icon" href="../assets/images/favicon-32x32.png"> -->
  <link rel="icon" href="">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./assets/js/fontawesome.all.min.js"></script>
  <script src="./assets/js/bulma-carousel.min.js"></script>
  <script src="./assets/js/bulma-slider.min.js"></script>
  <script src="./assets/js/index.js"></script>
</head>
<body>

<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>
    </div>

  </div>
</nav> -->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <!-- <img src="assets/images/invad_1024.png" class="interpolation-image" alt="logo" width="30%"> -->
          <!-- <img src="assets/images/vitad_1024.png" class="interpolation-image" alt="logo" style="height: 20% !important;" width="30%"> -->
          <!-- <h1 class="title is-1 publication-title"><span style="color:#b02418; font-weight:bold;">InvAD:</span> Learning Feature Inversion for Multi-class Unsupervised Anomaly Detection under General-purpose COCO-AD Benchmark</h1> -->
          <img src="assets/images/logo.png" class="interpolation-image" alt="logo" width="100%">
          <div class="is-size-5 publication-authors">
            <span class="author-block"> <a href="https://scholar.google.com/citations?user=J9lTFAUAAAAJ&hl=en">Yuxuan Cai</a><sup>1</sup>, </span>
            <span class="author-block"> <a href="https://scholar.google.com/citations?user=2hA4X9wAAAAJ&hl=en">Jiangning Zhangâ€ </a><sup>2</sup>, </span>
            <span class="author-block"> <a href="https://scholar.google.com/citations?user=m3KDreEAAAAJ&hl=en">Zhucun Xue</a><sup>2</sup>, </span>
            <span class="author-block"> <a href="https://scholar.google.com/citations?user=fa4NkScAAAAJ&hl=en&oi=ao">Zhenye Gan</a><sup>3</sup>, </span>
            <span class="author-block"> <a href="https://scholar.google.com/citations?user=gUJWww0AAAAJ&hl=en&oi=ao">Qingdong He</a><sup>3</sup>, </span>
            <span class="author-block"> <a href="https://scholar.google.com/citations?user=3lMuodUAAAAJ&hl=en&oi=ao">Xiaobin Hu</a><sup>3</sup>, </span>
            <span class="author-block"> <a href="https://scholar.google.com/citations?user=-OxQlHsAAAAJ&hl=en&oi=ao">Junwei Zhu</a><sup>3</sup>, </span> 
            <span class="author-block"> <a href="https://scholar.google.com/citations?user=xiK4nFUAAAAJ&hl=en">Yabiao Wang</a><sup>3</sup>, </span>
            <span class="author-block"> <a href="https://scholar.google.com/citations?user=fqte5H4AAAAJ&hl=en">Chengjie Wang</a><sup>3</sup>, </span> 
            <span class="author-block"> <a href="https://scholar.google.com/citations?user=YSIe_24AAAAJ&hl=en&oi=ao">Xinwei He</a><sup>4</sup>, </span>
            <span class="author-block"> <a href="https://scholar.google.com/citations?user=UeltiQ4AAAAJ&hl=en&oi=ao">Xiang Bai</a><sup>1</sup>, </span>

          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Huazhong University of Science and Technology &nbsp </span>
            <span class="author-block"><sup>2</sup>Zhejiang University &nbsp </span>
            <span class="author-block"><sup>3</sup>Youtu Lab, Tencent &nbsp </span>
            <span class="author-block"><sup>4</sup>Huazhong Agricultural University &nbsp </span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <!-- <span class="link-block">
                <a href="https://arxiv.org/abs/2404.10760"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span> -->
              <!-- <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=KPh62pfSHLQ"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/Fantasyele/HV-MMBench"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/datasets/ccaiyuxuan/HVMMBench/tree/main"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="section">
  <div class="container" style="margin-bottom: 2vh;">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Highlights</h2>
        <div class="content has-text-justified">
          <p>
          Compared to existing human-centric video benchmarks, our work offers the following key features: <br>
          <strong>(1) Diverse evaluation dimensions:</strong> HV-MMBench encompasses 14 tasks, ranging from basic attribute perception (e.g., age estimation, emotion recognition) to advanced cognitive reasoning (e.g., social relationship prediction, intention prediction), enabling comprehensive assessment of model capabilities; <br>
          <strong>(2) Varied data types:</strong> The benchmark includes multiple-choice, cloze, and open-ended question formats, combined with diverse evaluation metrics, to more accurately and robustly reflect model performance; <br>
          <strong>(3) Multi-domain video coverage:</strong> The benchmark spans 50 distinct visual scenarios, ensuring broad scene generalization; <br>
          <strong>(4) Temporal coverage:</strong> The benchmark covers videos from short-term (10 seconds) to long-term (up to 30min) durations, facilitating evaluation of MLLMs' abilities to capture contextual dynamics across varying temporal scales. <br>
          We evaluate several advanced open-source MLLMs on the HV-MMBench. While models excel in closed-form tasks, their performance drops sharply in open-ended generation, revealing a reliance on shallow patterns over genuine reasoning. In contrast, cloze and open-ended formats better expose reasoning challenges in human behavior understanding. By spanning diverse tasks and paradigms, HV-MMBench systematically reveals these limitations and facilitates the MLLM development. 
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
    
    <div class="columns is-centered">
      <div class="column is-12 has-text-centered">
        <img src="assets/images/statistic.png" style="width: 50%; height: auto;" alt="Statistic image">
      </div>
    </div>

    <div class="columns is-centered">
      <div class="column">
        <div class="content has-text-centered">
          <img src="assets/images/compare_table.png" style="width: 50%; height: auto;" alt="Statistic table">
          <p style="text-align: center; margin-left: auto; margin-right: auto; width: 50%;" >
            Comparison of prominent benchmarks in the video domain. We highlight key attributes, including domain scope (Open vs. Human), the number of videos (\#Videos), 
            the number of question-answer instances (\#QA Ins.), supported tasks, QA formats, evaluation metrics and resolution (Res). 
            MC, FIB, TF, and OEQ are the abbreviation of multiple-choice, fill-in-the-blank, true/false, and open-ended questions, respectively.
          </p>
        </div>
      </div>
    </div>
    

</div>
</section>

  <!-- Highlights Section -->
<!-- <section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <h2 class="title is-3">High-lights</h2>
        <div class="content has-text-justified">
          <p>
            <strong>1)</strong> Including 14 human-centric tasks, ranging from basic attribute perception to advanced cognitive reasoning. <br>
            <strong>2)</strong> Including multiple-choice, cloze, and open-ended question formats, combined with diverse evaluation metrics.
          </p>
        </div>
      </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-12 has-text-centered">
        <img src="assets/images/intro.png" style="width: 100%; height: auto;" alt="Intro image">
      </div>
    </div>
    <div class="columns is-centered">
      <div class="column is-12 has-text-centered">
        <img src="assets/images/pipeline.png" style="width: 100%; height: auto;" alt="Pipeline image">
      </div>
    </div>

  </div>
</section> -->


<!-- RESULTS SECTION -->
<section class="hero is-light is-small">
  <div class="hero-body has-text-centered">
    <h1 class="title is-1 mathvista_other">Experiments</h1>
  </div>
</section>

<section class="section">
  <div class="container">

    <div class="columns is-centered m-6">
      <div class="column is-full has-text-centered content">
        <h2 class="title is-3">Performance of different open-sourced MLLMs</h2>

          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="assets/images/exp_mc_tf.png" alt="" width="80%"/>
              <p>Performance of different open-sourced MLLMs on HV-MMBench under the Multiple-Choice and True/False questions, respectively.</p>
            </div>
          </div>

      </div>
    </div>

    <div class="columns is-centered m-6">
      <div class="column is-full has-text-centered content">
        <h2 class="title is-3">Performance of different open-sourced MLLMs</h2>

          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="assets/images/exp_FiB.png" alt="" width="80%"/>
              <p>Performance of MLLMs on HV-MMBench under the fill-in-blank questions.</p>
            </div>
          </div>

      </div>
    </div>
  
    <div class="columns is-centered m-6">
      <div class="column is-full has-text-centered content">
        <h2 class="title is-3">Performance of different open-sourced MLLMs</h2>

          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="assets/images/exp_oe.png" alt="" width="80%"/>
              <p>Performance of MLLMs on HV-MMBench under the open-ended questions.</p>
            </div>
          </div>

      </div>
    </div>

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
      @article{HV-MMBench,
        title={HV-MMBench: Benchmarking MLLMs for Human-Centric Video Understanding},
        author={Yuxuan Cai and Jiangning Zhang and Zhenye Gan and Qingdong He and Xiaobin Hu and Junwei Zhu and Yabiao Wang and Chengjie Wang and Zhucun Xue and Xinwei He and Xiang Bai},
        journal={},
        year={2025}
      }
    </code></pre>
  </div>
</section>


<!-- <footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./assets/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer> -->

<footer class="footer">
  <!-- <div class="container"> -->
    <div class="content has-text-centered">
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p style="text-align: center;">
            This website is adapted from <a href="https://video-mme.github.io/">Video-MME</a>, licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  <!-- </div> -->
</footer>

</body>
</html>

